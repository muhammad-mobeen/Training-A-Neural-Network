{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku94AwUOMzt0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy for math, pandas for data manipulation and matplotlib for data visualization.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for training neural network, pytorch library is used.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "UIrmP8kjNFXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if the GPU is available use it for the computation otherwise use the CPU.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kpsy2yB5NHCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There are 2 ways to load the Fashion MNIST dataset.\n",
        "\n",
        "#There are 2 ways to load the Fashion MNIST dataset.\n",
        "\n",
        "#1.   Load csv and then inherite Pytorch Dataset class .\n",
        "#2.   Use Pytorch module torchvision.datasets. It has many popular datasets like MNIST, FashionMNIST, CIFAR10 e.t.c\n",
        "\n",
        "#Note: We use DataLoader class from torch.utils.data to load data in batches in both method."
      ],
      "metadata": {
        "id": "vXHhtgPVNHPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code initializes two variables, train_set and test_set, with the FashionMNIST dataset.\n",
        "# Using FashionMNIST class from torchvision module.\n",
        "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()]))"
      ],
      "metadata": {
        "id": "3WjtdQaVNHaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the length and the answer is 60000\n",
        "len(train_set)"
      ],
      "metadata": {
        "id": "ZcKwoD1fNHvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code you provided sets up data loaders for the FashionMNIST dataset in PyTorch.\n",
        "#  Data loaders are a convenient way to load and iterate over mini-batches of data during the training and evaluation of a model.\n",
        "train_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                           batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=100)"
      ],
      "metadata": {
        "id": "FetWHs6ENIir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len is 600\n",
        "len(train_loader)"
      ],
      "metadata": {
        "id": "AJxVjM4lNJnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_label(label):\n",
        "    # Mapping of numeric labels to string descriptions\n",
        "    output_mapping = {\n",
        "        0: \"T-shirt/Top\",\n",
        "        1: \"Trouser\",\n",
        "        2: \"Pullover\",\n",
        "        3: \"Dress\",\n",
        "        4: \"Coat\",\n",
        "        5: \"Sandal\",\n",
        "        6: \"Shirt\",\n",
        "        7: \"Sneaker\",\n",
        "        8: \"Bag\",\n",
        "        9: \"Ankle Boot\"\n",
        "    }\n",
        "    \n",
        "    # Check if the label is a tensor, and if so, extract the scalar value\n",
        "    input = (label.item() if type(label) == torch.Tensor else label)\n",
        "    \n",
        "    # Return the corresponding string description from the mapping\n",
        "    return output_mapping[input]\n"
      ],
      "metadata": {
        "id": "b3b9HTiINJ4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Building a CNN\n",
        "\"\"\"\n",
        "Make a model class (FashionCNN in our case)\n",
        "It inherit nn.Module class that is a super class for all the neural networks in Pytorch.\n",
        "Our Neural Net has following layers:\n",
        "\n",
        "Two Sequential layers each consists of following layers-\n",
        "\n",
        "Convolution layer that has kernel size of 3 * 3, padding = 1 (zero_padding) in 1st layer and padding = 0 in second one. Stride of 1 in both layer.\n",
        "Batch Normalization layer.\n",
        "Acitvation function: ReLU.\n",
        "Max Pooling layer with kernel size of 2 * 2 and stride 2.\n",
        "Flatten out the output for dense layer(a.k.a. fully connected layer).\n",
        "3 Fully connected layer with different in/out features.\n",
        "1 Dropout layer that has class probability p = 0.25.\n",
        "All the functionaltiy is given in forward method that defines the forward pass of CNN.\n",
        "Our input image is changing in a following way:\n",
        "First Convulation layer : input: 28 * 28 * 3, output: 28 * 28 * 32\n",
        "First Max Pooling layer : input: 28 * 28 * 32, output: 14 * 14 * 32\n",
        "Second Conv layer : input : 14 * 14 * 32, output: 12 * 12 * 64\n",
        "Second Max Pooling layer : 12 * 12 * 64, output: 6 * 6 * 64\n",
        "Final fully connected layer has 10 output features for 10 types of clothes.\n",
        "Lets implementing the network...\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoMT1KkpcpNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    # Define the FashionCNN class as a subclass of nn.Module\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        # Initialize the FashionCNN class and its parent class (nn.Module)\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Define the first layer of the network as a sequence of operations:\n",
        "        # - Convolutional layer with 1 input channel, 32 output channels, and a 3x3 kernel with padding of 1\n",
        "        # - Batch normalization layer for normalization and stabilization\n",
        "        # - ReLU activation function for introducing non-linearity\n",
        "        # - Max pooling layer with a kernel size of 2x2 and stride of 2\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Define the second layer of the network as a sequence of operations:\n",
        "        # - Convolutional layer with 32 input channels, 64 output channels, and a 3x3 kernel\n",
        "        # - Batch normalization layer\n",
        "        # - ReLU activation function\n",
        "        # - Max pooling layer with a kernel size of 2x2\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        # Define the first fully connected (linear) layer with 64*6*6 input features and 600 output features\n",
        "        \n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        # Define a dropout layer with a dropout probability of 0.25\n",
        "        \n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        # Define the second fully connected (linear) layer with 600 input features and 120 output features\n",
        "        \n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        # Define the third fully connected (linear) layer with 120 input features and 10 output features\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the network\n",
        "        \n",
        "        out = self.layer1(x)\n",
        "        # Pass the input through the first layer\n",
        "        \n",
        "        out = self.layer2(out)\n",
        "        # Pass the output of the first layer through the second layer\n",
        "        \n",
        "        out = out.view(out.size(0), -1)\n",
        "        # Reshape the output into a 1D tensor by flattening it\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        # Pass the flattened output through the first fully connected layer\n",
        "        \n",
        "        out = self.drop(out)\n",
        "        # Apply dropout to the output\n",
        "        \n",
        "        out = self.fc2(out)\n",
        "        # Pass the output through the second fully connected layer\n",
        "        \n",
        "        out = self.fc3(out)\n",
        "        # Pass the output through the third fully connected layer\n",
        "        \n",
        "        return out\n",
        "        # Return the final output\n"
      ],
      "metadata": {
        "id": "AuwBDm8ScpYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a model of our CNN class\n",
        "\"\"\"\n",
        "Creating a object(model in the code)\n",
        "Transfering it into GPU if available.\n",
        "Defining a Loss function. we're using CrossEntropyLoss() here.\n",
        "Using Adam algorithm for optimization purpose.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1ZgLzQ9_cps8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    # Define the FashionCNN class as a subclass of nn.Module\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        # Initialize the FashionCNN class and its parent class (nn.Module)\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Define the first layer of the network as a sequence of operations:\n",
        "        # - Convolutional layer with 1 input channel, 32 output channels, and a 3x3 kernel with padding of 1\n",
        "        # - Batch normalization layer for normalization and stabilization\n",
        "        # - ReLU activation function for introducing non-linearity\n",
        "        # - Max pooling layer with a kernel size of 2x2 and stride of 2\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Define the second layer of the network as a sequence of operations:\n",
        "        # - Convolutional layer with 32 input channels, 64 output channels, and a 3x3 kernel\n",
        "        # - Batch normalization layer\n",
        "        # - ReLU activation function\n",
        "        # - Max pooling layer with a kernel size of 2x2\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        # Define the first fully connected (linear) layer with 64*6*6 input features and 600 output features\n",
        "        \n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        # Define a dropout layer with a dropout probability of 0.25\n",
        "        \n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        # Define the second fully connected (linear) layer with 600 input features and 120 output features\n",
        "        \n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        # Define the third fully connected (linear) layer with 120 input features and 10 output features\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the network\n",
        "        \n",
        "        out = self.layer1(x)\n",
        "        # Pass the input through the first layer\n",
        "        \n",
        "        out = self.layer2(out)\n",
        "        # Pass the output of the first layer through the second layer\n",
        "        \n",
        "        out = out.view(out.size(0), -1)\n",
        "        # Reshape the output into a 1D tensor by flattening it\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        # Pass the flattened output through the first fully connected layer\n",
        "        \n",
        "        out = self.drop(out)\n",
        "        # Apply dropout to the output\n",
        "        \n",
        "        out = self.fc2(out)\n",
        "        # Pass the output through the second fully connected layer\n",
        "        \n",
        "        out = self.fc3(out)\n",
        "        # Pass the output through the third fully connected layer\n",
        "        \n",
        "        return out\n",
        "        # Return the final output\n"
      ],
      "metadata": {
        "id": "I2ZRyzy7cpvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = FashionCNN()\n",
        "model.to(device)\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "pSeA3fNphSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "count = 0\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Lists for knowing classwise accuracy\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        # Transfering images and labels to GPU if available\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        train = Variable(images.view(100, 1, 28, 28))\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(train)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Propagating the error backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizing the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):    # It's the same as \"if count % 50 == 0\"\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels_list.append(labels)\n",
        "\n",
        "                test = Variable(images.view(100, 1, 28, 28))\n",
        "\n",
        "                outputs = model(test)\n",
        "\n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                predictions_list.append(predictions)\n",
        "                correct += (predictions == labels).sum()\n",
        "\n",
        "                total += len(labels)\n",
        "\n",
        "            accuracy = correct * 100 / total\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        if not (count % 500):\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz766srzcpz-",
        "outputId": "ccdd37ed-583f-4c07-a99c-19e217084c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(iteration_list, loss_list)  # Plotting the iteration numbers on the x-axis and loss values on the y-axis\n",
        "plt.xlabel(\"No. of Iteration\")  # Adding a label for the x-axis\n",
        "plt.ylabel(\"Loss\")  # Adding a label for the y-axis\n",
        "plt.title(\"Iterations vs Loss\")  # Adding a title for the plot\n",
        "plt.show()  # Displaying the plot\n"
      ],
      "metadata": {
        "id": "OGbD4bpDcp27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(iteration_list, accuracy_list)  # Plotting the iteration numbers on the x-axis and accuracy values on the y-axis\n",
        "plt.xlabel(\"No. of Iteration\")  # Adding a label for the x-axis\n",
        "plt.ylabel(\"Accuracy\")  # Adding a label for the y-axis\n",
        "plt.title(\"Iterations vs Accuracy\")  # Adding a title for the plot\n",
        "plt.show()  # Displaying the plot\n"
      ],
      "metadata": {
        "id": "GKO8oievcp5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking the Accuracy in each class of FashionMNIST dataset\n",
        "\n",
        "# Create lists to store the number of correct predictions for each class and the total number of predictions for each class\n",
        "class_correct = [0. for _ in range(10)]\n",
        "total_correct = [0. for _ in range(10)]\n",
        "\n",
        "# Disable gradient calculation to speed up the computation and reduce memory usage\n",
        "with torch.no_grad():\n",
        "    # Iterate over the test_loader to get images and labels\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        test = Variable(images)\n",
        "        outputs = model(test)\n",
        "        predicted = torch.max(outputs, 1)[1]\n",
        "        c = (predicted == labels).squeeze()\n",
        "\n",
        "        # Iterate over each prediction in the batch\n",
        "        for i in range(100):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            total_correct[label] += 1\n",
        "\n",
        "# Iterate over each class\n",
        "for i in range(10):\n",
        "    # Calculate the accuracy for each class and print it\n",
        "    print(\"Accuracy of {}: {:.2f}%\".format(output_label(i), class_correct[i] * 100 / total_correct[i]))\n"
      ],
      "metadata": {
        "id": "qk2-btIScp74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xu3hhtOcp-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4w4MKaJcqCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2TizwEscqFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g664mjsqcqIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVV_fqDycqLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZDgCbhfcqOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZrv-J9rcqQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zX9v6CT5cqTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87mINNhccqWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVVA9JG-cqZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISdzhx2zcqdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJrMwB0acqga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}